{"nbformat": 4, "nbformat_minor": 0, "metadata": {
    "language_info": {
        "name": "scala"
    },
    "kernelspec": {
        "name": "spark",
        "display_name": "Spark 1.1.0 (Scala 2.10.4)"
    }
}, "cells": [
    {
        "cell_type": "markdown",
        "source": "***Load Charmader Utilities***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "import org.att.charmander.CharmanderUtils",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "import org.att.charmander.CharmanderUtils"
                },
                "metadata": {},
                "execution_count": 13
            }
        ],
        "execution_count": 13
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***See what tasks are currently running and pick first test***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "val runningSimulators = CharmanderUtils.getMeteredTaskNamesFromRedis\nval simulatorToInvestigate = runningSimulators(0)",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "runningSimulators: List[String] = List(stress60)\nsimulatorToInvestigate: String = stress60"
                },
                "metadata": {},
                "execution_count": 14
            }
        ],
        "execution_count": 14
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***Retrieve memory-usage datapoints for the first simulator***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "val memoryUsage= CharmanderUtils.getRDDForTask(sc, simulatorToInvestigate, \"memory_usage\", 100)",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "memoryUsage: org.apache.spark.rdd.RDD[List[BigDecimal]] = stress60 ParallelCollectionRDD[23] at parallelize at CharmanderUtils.scala:141"
                },
                "metadata": {},
                "execution_count": 15
            }
        ],
        "execution_count": 15
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***Just for curiosity lets look at the first data point***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "memoryUsage.first",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "res19: List[BigDecimal] = List(1422585362000, 540000001, 33447936)"
                },
                "metadata": {},
                "execution_count": 16
            }
        ],
        "execution_count": 16
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***Translate Datapoints in to a Spark-SQL Database***",
        "metadata": {
            "collapsed": true
        }
    },
    {
        "cell_type": "code",
        "source": "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\nimport sqlContext._\n\ncase class MemoryUsage(timestamp: BigDecimal, memory: BigDecimal)\n\nval memoryusageRdd = memoryUsage.map(p => MemoryUsage(BigDecimal(p(0).asInstanceOf[BigInt]), BigDecimal(p(2).asInstanceOf[BigInt])))\nmemoryusageRdd.registerTempTable(\"memoryusage\")\n",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@40be3b0d\nimport sqlContext._\ndefined class MemoryUsage\nmemoryusageRdd: org.apache.spark.rdd.RDD[MemoryUsage] = MappedRDD[24] at map at <console>:83"
                },
                "metadata": {},
                "execution_count": 17
            }
        ],
        "execution_count": 17
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***Get max-memory usage***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "val newestMax = sqlContext.sql(\"select max(memory) from memoryusage\").first()",
        "metadata": {
            "collapsed": false,
            "trusted": true
        },
        "outputs": [
            {
                "output_type": "execute_result",
                "data": {
                    "text/plain": "newestMax: org.apache.spark.sql.Row = [63070208]"
                },
                "metadata": {},
                "execution_count": 18
            }
        ],
        "execution_count": 18
    },
    {
        "cell_type": "markdown",
        "source": "&nbsp;\n\n***Set the new max value in our task intelligence database***",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "CharmanderUtils.setTaskIntelligence(simulatorToInvestigate, \"mem\", newestMax(0).toString)",
        "metadata": {
            "collapsed": true,
            "trusted": true
        },
        "outputs": [],
        "execution_count": 21
    },
    {
        "cell_type": "markdown",
        "source": "***To Verify that the value is set***\n\nVisit [redis](http://172.31.2.11:31610/) and look for the task-intelligence section\n",
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": "",
        "metadata": {
            "collapsed": true,
            "trusted": true
        },
        "outputs": [],
        "execution_count": 20
    }
]}